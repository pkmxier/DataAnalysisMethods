{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pkmixer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pkmixer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pkmixer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_tags(words, tag_method='by-word'):   \n",
    "    tags = []\n",
    "    if tag_method == 'by-word':\n",
    "        for i in range(len(words)):\n",
    "            cur_tag = nltk.pos_tag([words[i]])[0][1][0]\n",
    "            tags.append(tags_dict.get(cur_tag, default))\n",
    "    elif tag_method == 'by-context':\n",
    "        tags = nltk.pos_tag(words)\n",
    "#         for i in range(len(tags)):\n",
    "#             tags[i] = tags_dict.get(tags[i][1], default)\n",
    "    \n",
    "    \n",
    "    return tags\n",
    "\n",
    "def get_lemmatized_words(lemmatizer, words):\n",
    "    tags = get_words_tags(words, tag_method='by-context')\n",
    "    \n",
    "    tags_dict = {\"J\": wordnet.ADJ,\n",
    "                 \"N\": wordnet.NOUN,\n",
    "                 \"V\": wordnet.VERB,\n",
    "                 \"R\": wordnet.ADV}\n",
    "    default = wordnet.NOUN\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for i in range(len(tags)):\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(tags[i][0], tags_dict.get(tags[i][1], default)))\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).translate(str.maketrans('', '', string.digits)).lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    return get_lemmatized_words(lemmatizer, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"bbc\"\n",
    "\n",
    "data = []\n",
    "target = []\n",
    "categories = os.listdir(path)\n",
    "\n",
    "for cat_index, category in enumerate(categories):\n",
    "    cur_path = os.path.join(path, category)\n",
    "    files = os.listdir(cur_path)\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(cur_path, file_name)\n",
    "        with open(file_path, encoding=\"utf8\", errors='ignore') as file:\n",
    "            data.append(file.read())\n",
    "            target.append(cat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        self.vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "        self.freq = self.vectorizer.fit_transform(x)\n",
    "\n",
    "        self.unique_categories = np.unique(y)\n",
    "        self.words_in_cat = np.zeros((self.unique_categories.size, self.freq.shape[1]))\n",
    "        self.num_cat = np.zeros(self.unique_categories.size)\n",
    "\n",
    "        for category in self.unique_categories:\n",
    "            self.words_in_cat[category] = np.sum(self.freq[category == y], axis=0)\n",
    "            self.num_cat[category] = self.freq[category == y].shape[0]\n",
    "\n",
    "    def predict_multinomial(self, documents):\n",
    "        result = []\n",
    "        for document in documents:\n",
    "            tokens = tokenizer(document)\n",
    "            cat_freq = np.zeros_like(self.num_cat, dtype=float)\n",
    "\n",
    "            for category in self.unique_categories:\n",
    "                cat_freq[category] = np.log(self.num_cat[category] / len(self.y))\n",
    "\n",
    "                for i in range(len(tokens)):\n",
    "                    word_freq = 0\n",
    "                    idx = self.vectorizer.vocabulary_.get(tokens[i], -1)\n",
    "                    if idx != -1:\n",
    "                        word_freq = self.words_in_cat[category, idx]\n",
    "\n",
    "                    cat_freq[category] += np.log((word_freq + 1) /\n",
    "                                                 (np.sum(self.words_in_cat[category]) + self.freq.shape[1]))\n",
    "\n",
    "            result.append(np.argmax(cat_freq))\n",
    "        return result\n",
    "\n",
    "    def predict_multidimensional(self, documents):\n",
    "        result = []\n",
    "        for document in documents:\n",
    "            tokens = tokenizer(document)\n",
    "            cat_freq = np.zeros_like(self.num_cat, dtype=float)\n",
    "\n",
    "            for category in self.unique_categories:\n",
    "                cat_freq[category] = np.log(self.num_cat[category] / len(self.y))\n",
    "                probability = 0\n",
    "                \n",
    "                for i in range(len(tokens)):\n",
    "                    word_freq = 0\n",
    "                    idx = self.vectorizer.vocabulary_.get(tokens[i], -1)\n",
    "                    if idx != -1:\n",
    "                        word_freq = self.words_in_cat[category, idx]\n",
    "                        probability = (word_freq + 1) / (np.sum(self.words_in_cat[category]) + 2)\n",
    "                    else:\n",
    "                        probability = 1 - (word_freq + 1) / (np.sum(self.words_in_cat[category]) + 2)\n",
    "                    \n",
    "                    cat_freq[category] += np.log(probability)\n",
    "\n",
    "            result.append(np.argmax(cat_freq))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in dataset: 2225\n",
      "Categories: ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
      "Multinoimal precision: 0.96213\n",
      "Multidimensional precision: 0.96855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def precision(pred, target):\n",
    "    return np.count_nonzero(np.equal(pred, target)) / len(pred)\n",
    "\n",
    "print(\"Documents in dataset: {}\\nCategories: {}\".format(len(data), categories))\n",
    "\n",
    "train, test, target_train, target_test = train_test_split(data, target, test_size=0.7, stratify=target)\n",
    "train, test, target_train, target_test = train[:], test[:], target_train[:], target_test[:]\n",
    "\n",
    "clf = NaiveBayes()\n",
    "clf.fit(train, target_train)\n",
    "\n",
    "\n",
    "pred_multinomial = clf.predict_multinomial(test)\n",
    "pred_multidimensinonal = clf.predict_multidimensional(test)\n",
    "\n",
    "print(\"Multinoimal precision: {:.5}\".format(precision(pred_multinomial, target_test)))\n",
    "print(\"Multidimensional precision: {:.5}\".format(precision(pred_multidimensinonal, target_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}